{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7wR5v1k9IKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import pickle\n",
        "import random\n",
        "from itertools import combinations\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9GCR6RZcBxJ",
        "colab_type": "code",
        "outputId": "833fa95f-f271-4229-9f8a-31d380b25347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxZ_P7m_UN30",
        "colab_type": "text"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqUqyYq9S74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1MqHRJi9w9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(\"data2.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HHNHj3d92_n",
        "colab_type": "code",
        "outputId": "1a8fa4b6-0572-417f-b47b-5d8a23e3c4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "raw_documents = []\n",
        "snippets = []\n",
        "with open( \"data/data.txt\", \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        text = line.strip()\n",
        "        raw_documents.append( text.lower() )\n",
        "        \n",
        "        snippets.append( text[0:min(len(text),100)] )\n",
        "print(\"Read %d raw text documents\" % len(raw_documents))\n",
        "\n",
        "\n",
        "# custom stopwords\n",
        "custom_stop_words = []\n",
        "with open( \"data/stopwords.txt\", \"r\" ) as f:\n",
        "    for line in f.readlines():\n",
        "        custom_stop_words.append( line.strip().lower() )\n",
        "        \n",
        "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 4551 raw text documents\n",
            "Stopword list has 350 entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHCbSQNUUL9",
        "colab_type": "text"
      },
      "source": [
        "# Some preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz52Xc29HdY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "dataset = [tokenizer.tokenize(x) for x in raw_documents]\n",
        "\n",
        "if not Path('vocab.pickle').exists():\n",
        "    vocab_freq = {}\n",
        "    for doc in dataset:\n",
        "        for word in doc:\n",
        "            if word in vocab_freq:\n",
        "                vocab_freq[word] += 1\n",
        "            else:\n",
        "                vocab_freq[word] = 1\n",
        "else:\n",
        "    with open('vocab.pickle', 'rb') as f:\n",
        "        vocab_freq = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l31Kpir0UiL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_list = [k for k, v in vocab_freq.items() if v > 50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzG021msVDVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "dataset = [tokenizer.tokenize(x) for x in raw_documents]\n",
        "for i, doc in enumerate(dataset):\n",
        "    dataset[i] = list(filter(lambda x: x in vocab_list and x not in custom_stop_words, doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QaGvdvaUlGM",
        "colab_type": "text"
      },
      "source": [
        "# Load pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBm0xFqWKD8l",
        "colab_type": "code",
        "outputId": "c958e8a6-dba2-4614-e2f4-413639bb3d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-20 12:26:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-20 12:26:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-20 12:26:53--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.06MB/s    in 6m 29s  \n",
            "\n",
            "2020-03-20 12:33:23 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qv1PpFHKKK_",
        "colab_type": "code",
        "outputId": "b265afbe-ef0d-428c-8c14-ef7c5a1d51c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "if not Path('emb_word2vec_format.txt').exists():\n",
        "    glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"emb_word2vec_format.txt\")\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('emb_word2vec_format.txt')\n",
        "weights = torch.FloatTensor(model.vectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "194jzguZYQCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = {k:i for i, k in enumerate(model.vocab.keys())}\n",
        "weight = np.array([model[k] for _, k in enumerate(model.vocab.keys())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI81C_h9aodF",
        "colab_type": "text"
      },
      "source": [
        "# Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACDBvyD2EelT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode = lambda x: word2idx[x] if x in model.vocab.keys() else word2idx['unk']\n",
        "dataset = [[encode(x) for x in y] for y in dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV-Vo6fBZQp9",
        "colab_type": "code",
        "outputId": "253be271-00cb-43ca-9a1e-35788fd1cbc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.hist([len(x) for x in dataset], bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSR0zBCSpY/8P0TtEs0kgslkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEMIU0S5bVpF",
        "colab_type": "code",
        "outputId": "301b7f28-c194-4fc6-b541-52057cd70588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(dataset))\n",
        "print(len(list(filter(lambda x: len(x) < 1024, dataset))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4551\n",
            "4278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HtrevTfbr_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 512\n",
        "tensor_dataset = []\n",
        "for doc in dataset:\n",
        "    tensor_dataset.append(torch.LongTensor(doc[:max_len]+(max_len - len(doc))*[encode('pad')]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yohdAUTWrSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_idx = word2idx['pad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTdUSyR_autB",
        "colab_type": "text"
      },
      "source": [
        "# Batch sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCzpQqne0ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSampler():\n",
        "    def __init__(self, dataset, negative_size=10, batch_size=5):\n",
        "        self.dataset = random.sample(dataset, len(dataset))\n",
        "        self.negative_size = negative_size\n",
        "        self.batch_size = batch_size\n",
        "        self.cur_idx = 0 \n",
        "\n",
        "    def __next__(self):\n",
        "        batch = {'positive': [], 'negative': []}\n",
        "        for j in range(self.batch_size):\n",
        "            positive = self.dataset[self.cur_idx]\n",
        "            rest = self.dataset[:self.batch_size*self.cur_idx+j] + self.dataset[self.batch_size*self.cur_idx+j+1:]\n",
        "            negative = torch.stack(random.sample(rest, self.negative_size), 0)\n",
        "            batch['positive'].append(positive)\n",
        "            batch['negative'].append(negative)\n",
        "        batch['positive'] = torch.stack(batch['positive'], 0)\n",
        "        batch['negative'] = torch.stack(batch['negative'], 0)\n",
        "          \n",
        "        return batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.cur_idx = 0\n",
        "        for _ in range(len(self.dataset) // self.batch_size):\n",
        "            self.cur_idx += 1\n",
        "            yield self.__next__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQeqN5CLayry",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdFohVtEm-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopicModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d, n_topics):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d = d\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d)\n",
        "        self.M_matrix = nn.Linear(d, d, bias=False)\n",
        "        self.proj = nn.Linear(d, n_topics)\n",
        "\n",
        "        self.T_matrix = nn.Parameter(nn.init.xavier_uniform_(torch.empty(n_topics, d)))\n",
        "\n",
        "    def load_embedding_weight(self, weight, padding_idx=None, freeze=False):\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.d).from_pretrained(weight, padding_idx=padding_idx)\n",
        "        if freeze == True:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        pos_emb = self.embedding(batch['positive'])\n",
        "        neg_context_emb = self.embedding(batch['negative']).mean(2)\n",
        "\n",
        "        sent_context = pos_emb.mean(1)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('ble,be->bl', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('ble,bl->be', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn), -1)\n",
        "        r = p @ self.T_matrix\n",
        "        \n",
        "        pos = torch.einsum('be,be->b', r, attn)\n",
        "        neg = torch.einsum('be,bme->bm', r, neg_context_emb)\n",
        "\n",
        "        return pos, neg \n",
        "    \n",
        "    def get_probs(self, inp):\n",
        "        pos_emb = self.embedding(inp)\n",
        "\n",
        "        sent_context = pos_emb.mean(0)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('le,e->l', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('le,l->e', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn))\n",
        "\n",
        "        return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pa2B7OAa09N",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMh7sngRl7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epoch = 15\n",
        "batch_size = 10\n",
        "negative_size = 20\n",
        "lamda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7eZKLP3McEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics_range = [3, 4, 5]\n",
        "topic_models = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLHF7IEmwEJL",
        "colab_type": "code",
        "outputId": "e99b70f0-7dbb-408e-cf1f-064799672ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "for n_topics in n_topics_range:\n",
        "    print(f'{n_topics} topics \\n')\n",
        "\n",
        "    topic_model = TopicModel(len(word2idx), 300, n_topics=n_topics)\n",
        "    topic_model.load_embedding_weight(torch.FloatTensor(weight), padding_idx=padding_idx, freeze=True)\n",
        "    topic_model.to(device)\n",
        "    optimizer = Adam(topic_model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    def topic_loss_function(pos, neg, model):\n",
        "        pos = pos.unsqueeze(-1)\n",
        "        delta = 1 - pos + neg\n",
        "        delta = F.relu(delta)\n",
        "        reg = torch.frobenius_norm(model.T_matrix @ model.T_matrix.permute(1,0)  - torch.eye(n_topics).to(pos.device))\n",
        "        loss = delta.sum() / batch_size + lamda * reg / len(sampler) / batch_size\n",
        "        return loss\n",
        "\n",
        "    topic_model.train()\n",
        "    for ep in range(n_epoch):\n",
        "        ep_loss = 0\n",
        "        sampler = DataSampler(tensor_dataset, negative_size=negative_size, batch_size=batch_size)\n",
        "        for step, batch in enumerate(iter(sampler)):\n",
        "            for k, v in batch.items():\n",
        "                batch[k] = v.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pos, neg = topic_model(batch)\n",
        "            loss = topic_loss_function(pos, neg, topic_model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ep_loss += loss.item()\n",
        "        scheduler.step(ep_loss)\n",
        "        print(f'Epoch {ep}, loss {ep_loss / len(sampler)}')\n",
        "\n",
        "    topic_model.eval()\n",
        "    with torch.no_grad():\n",
        "        W = []\n",
        "        for x in tensor_dataset:\n",
        "            W.append(topic_model.get_probs(x.to(device)).cpu().numpy())\n",
        "        W = np.array(W)\n",
        "\n",
        "        H = []\n",
        "        for x in vocab_list:\n",
        "            if x not in word2idx.keys():\n",
        "                continue\n",
        "            H.append(topic_model.get_probs(torch.tensor([word2idx[x]]).to(device)).cpu().numpy())\n",
        "        H = np.array(H)\n",
        "        H = H.transpose()\n",
        "\n",
        "    topic_models.append((n_topics, W, H))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics \n",
            "\n",
            "Epoch 0, loss 1.2757668178815749\n",
            "Epoch 1, loss 0.05699140659039635\n",
            "Epoch 2, loss 0.02544731321427357\n",
            "Epoch 3, loss 0.015426392713433726\n",
            "Epoch 4, loss 0.010602147052970275\n",
            "Epoch 5, loss 0.008985042452791726\n",
            "Epoch 6, loss 0.005846085080590386\n",
            "Epoch 7, loss 0.005054483994595952\n",
            "Epoch 8, loss 0.004317113649929045\n",
            "Epoch 9, loss 0.00414777402996321\n",
            "Epoch 10, loss 0.0034615964168211917\n",
            "Epoch 11, loss 0.048470345384359445\n",
            "Epoch 12, loss 0.00667789093484836\n",
            "Epoch 13, loss 0.0060544094526219176\n",
            "Epoch 14, loss 0.003633786946187144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 topics \n",
            "\n",
            "Epoch 0, loss 0.5957975877777694\n",
            "Epoch 1, loss 0.003851768232825734\n",
            "Epoch 2, loss 0.006993703343931094\n",
            "Epoch 3, loss 0.005690827013305542\n",
            "Epoch 4, loss 0.002368627160347507\n",
            "Epoch 5, loss 0.0017878309361749217\n",
            "Epoch 6, loss 0.023471742407751402\n",
            "Epoch 7, loss 0.014365355924646892\n",
            "Epoch 8, loss 0.0025896609508800883\n",
            "Epoch 9, loss 0.00097761982348287\n",
            "Epoch 10, loss 0.0009158142896082539\n",
            "Epoch 11, loss 0.0013887139848320856\n",
            "Epoch 12, loss 0.001705390617512371\n",
            "Epoch 13, loss 0.0009087988060315723\n",
            "Epoch 14, loss 0.000926125219480677\n",
            "5 topics \n",
            "\n",
            "Epoch 0, loss 0.8170271799732051\n",
            "Epoch 1, loss 0.02137350332373139\n",
            "Epoch 2, loss 0.008599114472783364\n",
            "Epoch 3, loss 0.002677136528878831\n",
            "Epoch 4, loss 0.004802201888370481\n",
            "Epoch 5, loss 0.0012883031881506463\n",
            "Epoch 6, loss 0.0018582774845596688\n",
            "Epoch 7, loss 0.001314638012375396\n",
            "Epoch 8, loss 0.0010468106614528122\n",
            "Epoch 9, loss 0.00713042127490453\n",
            "Epoch 10, loss 0.0010929377112476216\n",
            "Epoch 11, loss 0.008330347193763233\n",
            "Epoch 12, loss 0.002235742019243784\n",
            "Epoch 13, loss 0.006889870705512854\n",
            "Epoch 14, loss 0.0015142160987194914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oco_UCl3a4PP",
        "colab_type": "text"
      },
      "source": [
        "# Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wStKGxoms8U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = list(filter(lambda x: x in word2idx.keys(), vocab_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NUOC-drjLrQ",
        "colab_type": "code",
        "outputId": "7b4816af-5061-4cd9-cb8f-22a7b02f6b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "def get_descriptor( terms, H, topic_index, top ):\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    top_terms = []\n",
        "    for term_index in top_indices[0:top]:\n",
        "        top_terms.append( terms[term_index] )\n",
        "    return top_terms\n",
        "\n",
        "for (k,_,H) in topic_models:\n",
        "    print(f'{k} topics')\n",
        "    descriptors = []\n",
        "    for topic_index in range(k):\n",
        "        descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "        str_descriptor = \", \".join( descriptors[topic_index] )\n",
        "        print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics\n",
            "Topic 01: keeper, lloris, sturridge, goalkeeper, deflected, session, brake, front, minute, tabloid\n",
            "Topic 02: natwest, force, operates, schools, command, educated, douglas, admissions, units, secondary\n",
            "Topic 03: melania, renzi, flowers, chart, pink, dele, billboard, id, rocky, sigurdsson\n",
            "\n",
            "\n",
            "4 topics\n",
            "Topic 01: fca, obr, advert, watchdog, nab, fined, gueye, hashtag, banned, buzzfeed\n",
            "Topic 02: i, we, you, very, feel, what, really, things, good, lot\n",
            "Topic 03: klopp, fellaini, repeal, pardew, so., urges, 2017, hollande, signalled, guardiola\n",
            "Topic 04: group., dele, deeney, lamela, lallana, 5., karanka, in., thinktank, darmian\n",
            "\n",
            "\n",
            "5 topics\n",
            "Topic 01: released, statement, release, report, press, president, interview, comment, official, remarks\n",
            "Topic 02: average, %, population, 0.5, grew, index, gdp, grow, surveyed, below\n",
            "Topic 03: guidolin, kanté, in., lamela, 1., 5., lukaku, aguero, karanka, lingard\n",
            "Topic 04: wi-fi, 100m, payet, licence, revenant, beyoncé, puncheon, password, hashtag, long.\n",
            "Topic 05: zlatan, vardy, ibrahimovic, giroud, qc, refugee, lewandowski, bah, asic, paschi\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6essmo3jP-C",
        "colab_type": "code",
        "outputId": "3ebf6390-e959-475d-dde3-1f77330dcee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
        "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
        "    top_snippets = []\n",
        "    for doc_index in top_indices[0:top]:\n",
        "        top_snippets.append( all_snippets[doc_index] )\n",
        "    return top_snippets\n",
        "\n",
        "for (k,W,H) in topic_models:\n",
        "    print(f'{k} topics')\n",
        "    topic_snippets = get_top_snippets( snippets, W, 0, 10 )\n",
        "    for i, snippet in enumerate(topic_snippets):\n",
        "        print(\"%02d. %s\" % ( (i+1), snippet ) )\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics\n",
            "01. Moonlight and American Honey lead Film Independent Spirit awards nominations Andrea Arnold’s America\n",
            "02. Mrs Cameron’s Diary: Sarah’s like, hey bff - can we have Govey’s bongos back? Well I said to Mummy a\n",
            "03. Five of the best... rock & pop gigs DJ Shadow Are we in the midst of a sampladelic revival? You’d be\n",
            "04. Lissie: My Wild West review – heartfelt songs of pain, solidarity and rural retreat Illinois singer \n",
            "05. Let’s Eat Grandma review – frighteningly inventive duo rip up girl-group cliches From their ominous \n",
            "06. Snowden review – whistling in the wind For a director who customarily tackles subjects with the appr\n",
            "07. Cat’s Eyes: Treasure House review – celestial delicacy and gruesome horror As is custom with duos, C\n",
            "08. MGM reports $47.8m loss from Ben-Hur box-office flop MGM’s expensive remake of Ben-Hur lost the comp\n",
            "09. Real-life ice cream van battle set for big-screen treatment Get ready for The Cold War: a movie abou\n",
            "10. Public Access TV: Never Enough review – a debut fans of rock'n'roll will love The big problem facing\n",
            "\n",
            "\n",
            "4 topics\n",
            "01. Deadpool review – crude superhero laughs “A fourth wall break inside a fourth wall break? That’s lik\n",
            "02. Le point de vue du sur le Brexit: lettre à l’Europe et à nos partenaires européen L’onde de choc du \n",
            "03. Disney to make live-action Peter Pan Disney will produce a live-action version of Peter Pan, followi\n",
            "04. Zika’s greatest ally is human intransigence The revenge of the viruses marches on. After bird flu an\n",
            "05. Letter: Dave Swarbrick obituary Dave Swarbrick regularly appeared at Fairport Convention’s Cropredy \n",
            "06. Katy Perry's Twitter account is hacked and song leaked Twitter’s top user and pop superstar Katy Per\n",
            "07. Justin Bieber and Skrillex sued over vocal loops on Sorry Justin Bieber and Skrillex are being sued \n",
            "08. Goldman Sachs CEO Lloyd Blankfein takes 4% cut in compensation for 2015 With his bank’s net income d\n",
            "09. NHS saves £600m in crackdown on agency fees The NHS has slashed more than £600m from the billions it\n",
            "10. Deutsche Bank shares slide again after fresh speculation over US penalty Uncertainty about the size \n",
            "\n",
            "\n",
            "5 topics\n",
            "01. Live music booking now There has been radio silence from artsy electropop performer Patrick Wolf sin\n",
            "02. Disney plans to make live-action Snow White remake of classic film Disney is developing a live-actio\n",
            "03. Amazon and Morrisons tie-up: a customer's guide What will Amazon sell? Amazon will offer customers h\n",
            "04. Lena Dunham joins team of directors of election day documentary Lena Dunham is among a large group o\n",
            "05. Five of the best... rock & pop gigs DJ Shadow Are we in the midst of a sampladelic revival? You’d be\n",
            "06. New band of the week: The Pheels (No 106) Hometown: Atlanta. The lineup: Curtis Fields and Phil Jone\n",
            "07. Ed Sheeran sued for allegedly copying Marvin Gaye classic Let's Get It On Ed Sheeran has been accuse\n",
            "08. Gene Wilder – five key performances Gene Wilder exemplified a certain kind of crazed comic intensity\n",
            "09. Letter: I featured in the first documentary Peter Morley made – in 1947 I went to the same progressi\n",
            "10. Metallica announce their first studio album for eight years Metallica have announced their first stu\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1G8fwkaUKJV",
        "colab_type": "code",
        "outputId": "e879be5a-f4d6-4b94-9cb0-4598122c65e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "def calculate_coherence( w2v_model, term_rankings):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
        "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
        "        # get the mean for all pairs in this topic\n",
        "        topic_score = sum(pair_scores) / len(pair_scores)\n",
        "        overall_coherence += topic_score\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)\n",
        "\n",
        "k_values = []\n",
        "coherences = []\n",
        "for (k,W,H) in topic_models:\n",
        "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
        "    term_rankings = []\n",
        "    for topic_index in range(k):\n",
        "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "    # Now calculate the coherence based on our Word2vec model\n",
        "    k_values.append( k )\n",
        "    coherences.append( calculate_coherence( model, term_rankings ) )\n",
        "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K=03: Coherence=0.1115\n",
            "K=04: Coherence=0.2504\n",
            "K=05: Coherence=0.2230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7W4JBKqZiiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}